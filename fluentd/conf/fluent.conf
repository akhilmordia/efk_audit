# fluentd/conf/fluent.conf

# <source>
#   @type tail
#   path "#{ENV['KIBANA_LOG_FILE_PATH']}"
#   read_from_head true
#   pos_file /home/taildb/kibana.audit.log.json.pos
#   tag kibana.audit.log.json
#   <parse>
#       @type json
#   </parse>
# </source>
#
# <source>
#   @type tail
#   path "#{ENV['ELASTICSEARCH_LOG_FILE_PATH']}"
#   read_from_head true
#   pos_file /home/taildb/elasticsearch.audit.log.json
#   tag elasticsearch.audit.log.json
#   <parse>
#       @type json
#   </parse>
# </source>

<source>
  @type forward
  port "#{ENV['FLUENTD_PORT']}"
  bind 0.0.0.0
  tag forward.log
</source>

<source>
  @type tail
  path /var/lib/docker/containers/*/*-json.log
  read_from_head true
  pos_file /home/taildb/audit.log.json
  tag all.audit.log.json
  <parse>
      @type json
  </parse>
</source>

<filter *.audit.log.json>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type json
  </parse>
</filter>

# <filter *.audit.log.json>
#   @type record_transformer
#   enable_ruby
#   <record>
#     timestamp ${record["timestamp"] || record["@timestamp"] || Time.now.strftime('%Y-%m-%dT%H:%M:%S.%N%z')}
#     level ${record["level"] || "debug"}
#   </record>
# </filter>

<filter **>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type multi_format
    <pattern>
      format json
    </pattern>
    <pattern>
      format regexp
      #expression ^\[(?<log_time>.*?)\] (?<message>.*?)$
      expression ^(?<message>.*?)$
    </pattern>
  </parse>
</filter>



# <filter kibana.audit.log.json>
#   @type record_transformer
#   enable_ruby
#   <record>
#     tag "kibana.audit.log.json"
#   </record>
# </filter>
#
# <filter elasticsearch.audit.log.json>
#   @type record_transformer
#   enable_ruby
#   <record>
#     tag "elasticsearch.audit.log.json"
#   </record>
# </filter>

<filter **>
  @type record_transformer
  enable_ruby
  <record>
    timestamp ${record["timestamp"] || record["@timestamp"] || Time.now.strftime('%Y-%m-%dT%H:%M:%S.%N%z')}
    level ${record["level"] || "debug"}
  </record>
</filter>


<match *.audit.log.json>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['ES_HOST']}"
    port "#{ENV['ES_PORT']}"
    user "#{ENV['ES_USERNAME']}"
    password "#{ENV['ES_PASSWORD']}"
    index_name "#{ENV['ES_STANDARD_LOGS_INDEX']}"
    flush_interval 2s
    reload_connections false
    reconnect_on_error true
    reload_on_failure true
    suppress_type_name true
    log_es_400_reason true
    <buffer>
      flush_thread_count 8
      flush_interval 1s
      chunk_limit_size 32M
      queue_limit_length 4
      flush_mode interval
      retry_max_interval 30
      retry_forever true
    </buffer>
  </store>
  <store>
    @type stdout
  </store>
</match>

<match forward.log>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['ES_HOST']}"
    port "#{ENV['ES_PORT']}"
    user "#{ENV['ES_USERNAME']}"
    password "#{ENV['ES_PASSWORD']}"
    index_name "#{ENV['ES_STANDARD_LOGS_INDEX']}"
    flush_interval 2s
    reload_connections false
    reconnect_on_error true
    reload_on_failure true
    suppress_type_name true
    log_es_400_reason true
    <buffer>
      flush_thread_count 8
      flush_interval 1s
      chunk_limit_size 32M
      queue_limit_length 4
      flush_mode interval
      retry_max_interval 30
      retry_forever true
    </buffer>
  </store>
  <store>
    @type stdout
  </store>
</match>